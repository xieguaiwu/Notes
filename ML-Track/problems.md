# Problems

---

## 11.22-23

深度学习：自我训练，将数据集$\mathcal{D}$中的二元组重新输入$h(x)$，从而规避经验主义数据训练的边际效应递减原理

基于规则：一种经验主义的概率训练法基于一套简单的规则，即便计算语言学的思想会有一套略微复杂的初始训练，但这在本质上与经验主义的概率训练的初始规则有何区别？足量的训练会抹平可能的问题，而问题在于如何令训练不出现边际效应递减。

~~如何结合计算语言学和强化学习？~~

知识拓扑？

BERT语义向量

L_1/L_2约束？
